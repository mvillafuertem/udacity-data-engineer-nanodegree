{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bitdendcondacf54d56bf2ff408b9345a9eba404596b",
   "display_name": "Python 3.7.7 64-bit ('dend': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis of Weather data\n",
    "This notebook contains EDA of Weather data in order to gain information for the ETL process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('weather') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading one weather attribute and:\n",
    "* take a look at the dataframe schema\n",
    "* verify correct time period of the data\n",
    "* check basic statistical description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- datetime: timestamp (nullable = true)\n |-- Vancouver: double (nullable = true)\n |-- Portland: double (nullable = true)\n |-- San Francisco: double (nullable = true)\n |-- Seattle: double (nullable = true)\n |-- Los Angeles: double (nullable = true)\n |-- San Diego: double (nullable = true)\n |-- Las Vegas: double (nullable = true)\n |-- Phoenix: double (nullable = true)\n |-- Albuquerque: double (nullable = true)\n |-- Denver: double (nullable = true)\n |-- San Antonio: double (nullable = true)\n |-- Dallas: double (nullable = true)\n |-- Houston: double (nullable = true)\n |-- Kansas City: double (nullable = true)\n |-- Minneapolis: double (nullable = true)\n |-- Saint Louis: double (nullable = true)\n |-- Chicago: double (nullable = true)\n |-- Nashville: double (nullable = true)\n |-- Indianapolis: double (nullable = true)\n |-- Atlanta: double (nullable = true)\n |-- Detroit: double (nullable = true)\n |-- Jacksonville: double (nullable = true)\n |-- Charlotte: double (nullable = true)\n |-- Miami: double (nullable = true)\n |-- Pittsburgh: double (nullable = true)\n |-- Toronto: double (nullable = true)\n |-- Philadelphia: double (nullable = true)\n |-- New York: double (nullable = true)\n |-- Montreal: double (nullable = true)\n |-- Boston: double (nullable = true)\n |-- Beersheba: double (nullable = true)\n |-- Tel Aviv District: double (nullable = true)\n |-- Eilat: double (nullable = true)\n |-- Haifa: double (nullable = true)\n |-- Nahariyya: double (nullable = true)\n |-- Jerusalem: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "data_folder = '/Users/tomra/Projects/data-engineering/udacity-data-engineer-nanodegree/06-capstone-project/data'\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .load(os.path.join(data_folder, 'historical-hourly-weather-data', 'temperature.csv'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(datetime=datetime.datetime(2012, 10, 1, 12, 0))]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df.select('datetime').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(datetime=datetime.datetime(2017, 11, 30, 0, 0))]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df.select('datetime').sort('datetime', ascending=False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  summary             Chicago\n0   count               45250\n1    mean   283.3505727940784\n2  stddev  10.997137350331892\n3     min              248.89\n4     max              308.48",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>Chicago</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>45250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>283.3505727940784</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stddev</td>\n      <td>10.997137350331892</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>248.89</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>max</td>\n      <td>308.48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.select('datetime', 'Chicago').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             datetime          Chicago\n0 2012-10-01 12:00:00             None\n1 2012-10-01 13:00:00  overcast clouds\n2 2012-10-01 14:00:00  overcast clouds\n3 2012-10-01 15:00:00  overcast clouds\n4 2012-10-01 16:00:00  overcast clouds",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>Chicago</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2012-10-01 12:00:00</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012-10-01 13:00:00</td>\n      <td>overcast clouds</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012-10-01 14:00:00</td>\n      <td>overcast clouds</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-10-01 15:00:00</td>\n      <td>overcast clouds</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-10-01 16:00:00</td>\n      <td>overcast clouds</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .load(os.path.join(data_folder, 'historical-hourly-weather-data', 'weather_description.csv'))\n",
    "df.select('datetime', 'Chicago').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This dataset seems straightforward to use. We have data with one hour granularity. Since precipitation and taxi rides have finer grain we'll settle for summarizing the fact table on an hourly basis. The temperature seems to be in Kelvin, so a unit conversion is probably feasible to conduct for added usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}