{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone project\n",
    "## Data preparation for data analysis\n",
    "\n",
    "This notebook prepares data for develeoping machine learning models to give additional insights about the trips.\n",
    "\n",
    "* This is a overly simplified example of the data engineers role in the production pipeline.\n",
    "* In reality, several of these steps would be initially carried out by the data scientists (feature engineering).\n",
    "* When the model has been developed we can start integrating it into the data pipeline.\n",
    "* We'd possibly separate the data preparation and inference into own spark jobs that would be orchestrated in Airflow.\n",
    "* Results of the inference would be written to the data warehouse with the other data.\n",
    "\n",
    "PySpark EMR notebooks do not have pandas and matplotlib pre-installed, so we need to install them using the install_pypi_package function found in SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19eb1a61e4a4bc88593d70b7eff378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.3 python-dateutil-2.8.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1587285756158-0/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration settings for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66382a6c79c041e297ecfcd701add62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b46d09826c406d94e77ec929afd605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_conf = {\n",
    "    's3_bucket': 's3://dend-tomra',\n",
    "    's3_model_key': 'taxi_ml/model',\n",
    "    's3_data_key': 'taxi_ml/data',    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66ee100f4a64ce08726fafb744e8b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1587285063067_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-29-113.us-west-2.compute.internal:20888/proxy/application_1587285063067_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-26-204.us-west-2.compute.internal:8042/node/containerlogs/container_1587285063067_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "etl_conf = { \n",
    "    \"s3_taxi_dir_path\":\"s3://dend-tomra/chicago-taxi-rides-2016\",\n",
    "    \"s3_precip_file_path\":\"s3://dend-tomra/ghcnd/2016\",\n",
    "    \"s3_weather_dir_path\":\"s3://dend-tomra/historical-hourly-weather-data\",\n",
    "    \"s3_holidays_file_path\":\"s3://dend-tomra/US-Bank-holidays.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first step we'll settle on reading only one month's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c362d402b86f448da2ca32e1d3a50e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- taxi_id: integer (nullable = true)\n",
      " |-- trip_start_timestamp: timestamp (nullable = true)\n",
      " |-- trip_end_timestamp: timestamp (nullable = true)\n",
      " |-- trip_seconds: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- pickup_census_tract: string (nullable = true)\n",
      " |-- dropoff_census_tract: integer (nullable = true)\n",
      " |-- pickup_community_area: integer (nullable = true)\n",
      " |-- dropoff_community_area: integer (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- extras: double (nullable = true)\n",
      " |-- trip_total: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- company: integer (nullable = true)\n",
      " |-- pickup_latitude: integer (nullable = true)\n",
      " |-- pickup_longitude: integer (nullable = true)\n",
      " |-- dropoff_latitude: integer (nullable = true)\n",
      " |-- dropoff_longitude: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# use for only one file\n",
    "filename = 'chicago_taxi_trips_2016_01.csv'\n",
    "\n",
    "# use for reading all files\n",
    "# filename = '*'\n",
    "\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .load(os.path.join(etl_conf['s3_taxi_dir_path'], filename))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c36194720e4bc9a8d0767daf0a871e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   taxi_id trip_start_timestamp  ... dropoff_latitude  dropoff_longitude\n",
      "0       85  2016-01-13 06:15:00  ...            199.0              510.0\n",
      "1     2776  2016-01-22 09:30:00  ...              NaN                NaN\n",
      "2     3168  2016-01-31 21:30:00  ...              NaN                NaN\n",
      "3     4237  2016-01-23 17:30:00  ...            686.0              500.0\n",
      "4     5710  2016-01-14 05:45:00  ...              NaN                NaN\n",
      "\n",
      "[5 rows x 20 columns]"
     ]
    }
   ],
   "source": [
    "# Take a look at the top rows\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7bcc9f37e14ddf8863989dc94866a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705805"
     ]
    }
   ],
   "source": [
    "# Check initial number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "For this step we are assuming that the data scientists have also taken a look at the data and given us the information about what features (columns) to use in the model.\n",
    "Furthermore we're assuming that is this specific case we are developing a clustering algorithm to try to identify different customer profiles.\n",
    "\n",
    "The features of interest are:\n",
    "* Trip time of day (hour)\n",
    "* Pickup community area\n",
    "* Dropoff communicaty area\n",
    "\n",
    "In addition we want to include year, month, day for partitioning of the resulting data.\n",
    "\n",
    "The assumption is that data the analysists have found out that there is a strong correlation how pickup and dropoff areas affect trip duration, mileage and cost. Thus we can omit these features in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract hour\n",
    "We use the *hour* function from the pyspark.sql.functions library to get the hour from `trip_start_timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e185338d4743bb98ca1710948869b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_with_hour = df.withColumn('year', year(df.trip_start_timestamp))\\\n",
    "                 .withColumn('month', month(df.trip_start_timestamp))\\\n",
    "                 .withColumn('day', dayofmonth(df.trip_start_timestamp))\\\n",
    "                 .withColumn('hour', hour(df.trip_start_timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with wanted features\n",
    "We create a new DataFram only containing the features we will be using by the clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a700ebff784ccd86383c2a7b35ba99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_features = df_with_hour.select('year', 'month', 'day', 'hour', 'pickup_community_area', 'dropoff_community_area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values\n",
    "\n",
    "Machine learning models cannot handle null values, so we will need to do something with those. One option is to fill them with e.g. average values or similar. Since we're looking at start and end locations a decision is made that the only feasible option is to drop the values, since making wrong assumptions how the customer is travelling probably would end up in an erroneous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74623ac5d644e638087df48bb155d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_no_nulls = df_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36238654d1684e0ab8fa6a74d159509c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382311"
     ]
    }
   ],
   "source": [
    "df_no_nulls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical features\n",
    "Although the pickup and dropoff areas are conveniently already in numeric format we're not yet ready to pass them to a machine learning model. We could, but the results might not be what we expect. The reasonis that these features are considered categorical values meaning that they are used to indicate distinct values but the values in relation to each other is irrelevant. An area with value 100 is no better or worse than a value of 600, they merely belong to different categories. The same applies to hour data.\n",
    "\n",
    "This situation is common in data science and is handled useing a method called one-hot enconding. \n",
    "\n",
    "From the PySpark documentation:\n",
    "> A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bf3116810543baa1415b92569b7fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "community_area_encoder = OneHotEncoderEstimator() \\\n",
    "    .setInputCols(['hour', 'pickup_community_area', 'dropoff_community_area']) \\\n",
    "    .setOutputCols(['hour_encoded', 'pickup_community_area_encoded', 'dropoff_community_area_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac022f9e62a741ccac0502c30820448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "community_area_model = community_area_encoder.fit(df_no_nulls)\n",
    "df_encoded = community_area_model.transform(df_no_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ceb3c0e6613412995bd15483edb0913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- pickup_community_area: integer (nullable = true)\n",
      " |-- dropoff_community_area: integer (nullable = true)\n",
      " |-- hour_encoded: vector (nullable = true)\n",
      " |-- pickup_community_area_encoded: vector (nullable = true)\n",
      " |-- dropoff_community_area_encoded: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f8d56c3cf7426b973d550d15a56301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = output_conf['s3_bucket']\n",
    "key = output_conf['s3_model_key']\n",
    "encoder_name = 'community_area_encoder'\n",
    "model_name = 'community_area_model'\n",
    "\n",
    "encoder_path = os.path.join(bucket, key, encoder_name)\n",
    "community_area_encoder.write().overwrite().save(encoder_path)\n",
    "\n",
    "model_path = os.path.join(bucket, key, model_name)\n",
    "community_area_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the final DataFrame\n",
    "Finally, let's drop the unneeded columns and store the final dataframe to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570bdba4dbb549b19f03837bf2986d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final = df_encoded.select('year',\n",
    "                             'month',\n",
    "                             'day',\n",
    "                             'hour_encoded',\n",
    "                             'pickup_community_area_encoded',\n",
    "                             'dropoff_community_area_encoded'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140c6fea7c9493093ec1f611186ff4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = output_conf['s3_bucket']\n",
    "key = output_conf['s3_data_key']\n",
    "\n",
    "output_path = os.path.join(bucket, key)\n",
    "\n",
    "df_final.write.partitionBy('year', 'month', 'day') \\\n",
    "        .parquet(output_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "When running the script on whole dataset it failed because some of the pickup and dropoff data was in string format albeit being numberic (otherwise the main ETL would have failed since that uses numeric data type for said columns). As a result the resulting `data_prep.py` script has two additional steps that were not included here. I perform `StringIndexer` transformation to both pickup and dropoff columns so that all values are converted to numeric values before feeding them to the one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}